# Learning the language of DNA

## 学习DNA的语言

Link: https://www.science.org/doi/abs/10.1126/science.adt3007?af=R

Science, Volume 386, Issue 6723, Page 729-730, November 2024. <br />


---
# Harnessing deep learning to build optimized ligands

## 利用深度学习构建优化的配体

Link: https://www.nature.com/articles/s43588-024-00725-1

<p>Nature Computational Science, Published online: 14 November 2024; <a href="https://www.nature.com/articles/s43588-024-00725-1">doi:10.1038/s43588-024-00725-1</a></p>A recent study proposes DeepBlock, a deep learning-based approach for generating ligands with targeted properties, such as low toxicity and high affinity with the given target. This approach outperforms existing methods in the field while maintaining synthetic accessibility and drug-likeness.


---
# Fast and generalizable micromagnetic simulation with deep neural nets

## 基于深度神经网络的快速广义微磁模拟

Link: https://www.nature.com/articles/s42256-024-00914-7

<p>Nature Machine Intelligence, Published online: 14 November 2024; <a href="https://www.nature.com/articles/s42256-024-00914-7">doi:10.1038/s42256-024-00914-7</a></p>Many physical systems involve long-range interactions, which present a considerable obstacle to large-scale simulations. Cai, Li and Wang introduce NeuralMAG, a deep learning approach to reduce complexity and accelerate micromagnetic simulations.


---
# Hearable devices with sound bubbles

## 具有声音气泡的可听设备

Link: https://www.nature.com/articles/s41928-024-01276-z

<p>Nature Electronics, Published online: 14 November 2024; <a href="https://www.nature.com/articles/s41928-024-01276-z">doi:10.1038/s41928-024-01276-z</a></p>An intelligent headset system that uses real-time neural networks run on an embedded central processing unit can create sound bubbles that selectively isolate groups of users from outside sounds.


---
# [ASAP] How Sophisticated Are Neural Networks Needed to Predict Long-Term Nonadiabatic Dynamics?

## [ASAP] 预测长期非绝热动力学需要多复杂的神经网络？

Link: http://dx.doi.org/10.1021/acs.jctc.4c01223

<p><img alt="TOC Graphic" src="https://pubs.acs.org/cms/10.1021/acs.jctc.4c01223/asset/images/medium/ct4c01223_0010.gif" /></p><div><cite>Journal of Chemical Theory and Computation</cite></div><div>DOI: 10.1021/acs.jctc.4c01223</div>


---
# Accelerating data acquisition with FPGA-based edge machine learning: a case study with LCLS-II

## 使用基于FPGA的边缘机器学习加速数据采集: 以lcls-ii为例

Link: http://iopscience.iop.org/article/10.1088/2632-2153/ad8ea8

New scientific experiments and instruments generate vast amounts of data that need to be transferred for storage or further processing, often overwhelming traditional systems. Edge machine learning (EdgeML) addresses this challenge by integrating machine learning (ML) algorithms with edge computing, enabling real-time data processing directly at the point of data generation. EdgeML is particularly beneficial for environments where immediate decisions are required, or where bandwidth and storage are limited. In this paper, we demonstrate a high-speed configurable ML model in a fully customizable EdgeML system using a field programmable gate array (FPGA). Our demonstration focuses on an angular array of electron spectrometers, referred to as the ‘CookieBox,’ developed for the Linac Coherent Light Source II project. The EdgeML system captures 51.2 Gbps from a 6.4 GS s−1 analog to digital converter and is designed to integrate data pre-processing and ML inside an FPGA. Our implementation achieves an inference latency of 0.2 µs for the ML model, and a total latency of 0.4 µs for the complete EdgeML system, which includes pre-processing, data transmission, digitization, and ML inference. The modular design of the system allows it to be adapted for other instrumentation applications requiring low-latency data processing.


---
# Computationally Efficient Machine-Learned Model for GST Phase Change Materials via Direct and Indirect Learning

## 通过直接和间接学习实现GST相变材料的高效计算机器学习模型

Link: https://arxiv.org/abs/2411.08194

arXiv:2411.08194v1 Announce Type: new 
Abstract: Phase change materials such as Ge$_{2}$Sb$_{2}$Te$_{5}$ (GST) are ideal candidates for next-generation, non-volatile, solid-state memory due to the ability to retain binary data in the amorphous and crystal phases, and rapidly transition between these phases to write/erase information. Thus, there is wide interest in using molecular modeling to study GST. Recently, a Gaussian Approximation Potential (GAP) was trained for GST to reproduce Density Functional Theory (DFT) energies and forces at a fraction of the computational cost [Zhou et al. Nature Electronics $\mathbf{6}$, 746-754 (2023)]; however, simulations of large length and time scales are still challenging using this GAP model. Here we present a machine-learned (ML) potential for GST implemented using the Atomic Cluster Expansion (ACE) framework. This ACE potential shows comparable accuracy to the GAP potential but performs orders of magnitude faster. We train the ACE potentials both directly from DFT, as well as using a recently introduced indirect learning approach where the potential is trained instead from an intermediate ML potential, in this case, GAP. Indirect learning allows us to consider a significantly larger training set than could be generated using DFT alone. We compare the directly and indirectly learned potentials and find that both reproduce the structure and thermodynamics predicted by the GAP, and also match experimental measures of GST structure. The speed of the ACE model, particularly when using GPU acceleration, allows us to examine repeated transitions between crystal and amorphous phases in device-scale systems with only modest computational resources.


---
# Deep Learning Accelerated Quantum Transport Simulations in Nanoelectronics: From Break Junctions to Field-Effect Transistors

## 纳米电子学中的深度学习加速量子输运模拟: 从断裂结到场效应晶体管

Link: https://arxiv.org/abs/2411.08800

arXiv:2411.08800v1 Announce Type: new 
Abstract: Quantum transport calculations are essential for understanding and designing nanoelectronic devices, yet the trade-off between accuracy and computational efficiency has long limited their practical applications. We present a general framework that combines the deep learning tight-binding Hamiltonian (DeePTB) approach with the non-equilibrium Green's Function (NEGF) method, enabling efficient quantum transport calculations while maintaining first-principles accuracy. We demonstrate the capabilities of the DeePTB-NEGF framework through two representative applications: comprehensive simulation of break junction systems, where conductance histograms show good agreement with experimental measurements in both metallic contact and single-molecule junction cases; and simulation of carbon nanotube field effect transistors through self-consistent NEGF-Poisson calculations, capturing essential physics including the electrostatic potential and transfer characteristic curves under finite bias conditions. This framework bridges the gap between first-principles accuracy and computational efficiency, providing a powerful tool for high-throughput quantum transport simulations across different scales in nanoelectronics.


---
# MatPilot: an LLM-enabled AI Materials Scientist under the Framework of Human-Machine Collaboration

## MatPilot: 人机协作框架下的LLM AI材料科学家

Link: https://arxiv.org/abs/2411.08063

arXiv:2411.08063v1 Announce Type: cross 
Abstract: The rapid evolution of artificial intelligence, particularly large language models, presents unprecedented opportunities for materials science research. We proposed and developed an AI materials scientist named MatPilot, which has shown encouraging abilities in the discovery of new materials. The core strength of MatPilot is its natural language interactive human-machine collaboration, which augments the research capabilities of human scientist teams through a multi-agent system. MatPilot integrates unique cognitive abilities, extensive accumulated experience, and ongoing curiosity of human-beings with the AI agents' capabilities of advanced abstraction, complex knowledge storage and high-dimensional information processing. It could generate scientific hypotheses and experimental schemes, and employ predictive models and optimization algorithms to drive an automated experimental platform for experiments. It turns out that our system demonstrates capabilities for efficient validation, continuous learning, and iterative optimization.


---
# Material Property Prediction with Element Attribute Knowledge Graphs and Multimodal Representation Learning

## 基于元素属性知识图和多模态表示学习的材料性能预测

Link: https://arxiv.org/abs/2411.08414

arXiv:2411.08414v1 Announce Type: cross 
Abstract: Machine learning has become a crucial tool for predicting the properties of crystalline materials. However, existing methods primarily represent material information by constructing multi-edge graphs of crystal structures, often overlooking the chemical and physical properties of elements (such as atomic radius, electronegativity, melting point, and ionization energy), which have a significant impact on material performance. To address this limitation, we first constructed an element property knowledge graph and utilized an embedding model to encode the element attributes within the knowledge graph. Furthermore, we propose a multimodal fusion framework, ESNet, which integrates element property features with crystal structure features to generate joint multimodal representations. This provides a more comprehensive perspective for predicting the performance of crystalline materials, enabling the model to consider both microstructural composition and chemical characteristics of the materials. We conducted experiments on the Materials Project benchmark dataset, which showed leading performance in the bandgap prediction task and achieved results on a par with existing benchmarks in the formation energy prediction task.


---
# On the numerical integration of the Fokker-Planck equation driven by a mechanical force and the Bismut-Elworthy-Li formula

## 关于机械力驱动的fokker-planck方程和bismut-elworth-li公式的数值积分

Link: https://arxiv.org/abs/2411.08518

arXiv:2411.08518v1 Announce Type: cross 
Abstract: Optimal control theory aims to find an optimal protocol to steer a system between assigned boundary conditions while minimizing a given cost functional in finite time. Equations arising from these types of problems are often non-linear and difficult to solve numerically. In this note, we describe numerical methods of integration for two partial differential equations that commonly arise in optimal control theory: the Fokker-Planck equation driven by a mechanical potential for which we use Girsanov theorem; and the Hamilton-Jacobi-Bellman, or dynamic programming, equation for which we find the gradient of its solution using the Bismut-Elworthy-Li formula. The computation of the gradient is necessary to specify the optimal protocol. Finally, we give an example application in solving an optimal control problem without spacial discretization by machine learning.


---
# UniMat: Unifying Materials Embeddings through Multi-modal Learning

## UniMat: 通过多模态学习统一材料嵌入

Link: https://arxiv.org/abs/2411.08664

arXiv:2411.08664v1 Announce Type: cross 
Abstract: Materials science datasets are inherently heterogeneous and are available in different modalities such as characterization spectra, atomic structures, microscopic images, and text-based synthesis conditions. The advancements in multi-modal learning, particularly in vision and language models, have opened new avenues for integrating data in different forms. In this work, we evaluate common techniques in multi-modal learning (alignment and fusion) in unifying some of the most important modalities in materials science: atomic structure, X-ray diffraction patterns (XRD), and composition. We show that structure graph modality can be enhanced by aligning with XRD patterns. Additionally, we show that aligning and fusing more experimentally accessible data formats, such as XRD patterns and compositions, can create more robust joint embeddings than individual modalities across various tasks. This lays the groundwork for future studies aiming to exploit the full potential of multi-modal data in materials science, facilitating more informed decision-making in materials design and discovery.


---
# Deep learning tight-binding approach for large-scale electronic simulations at finite temperatures with $ab$ $initio$ accuracy

## 有限温度下大规模电子模拟的深度学习紧绑定方法，具有 $ ab $ initio $ 精度

Link: https://arxiv.org/abs/2307.04638

arXiv:2307.04638v3 Announce Type: replace 
Abstract: Simulating electronic behavior in materials and devices with realistic large system sizes remains a formidable task within the $ab$ $initio$ framework due to its computational intensity. Here we show DeePTB, an efficient deep learning-based tight-binding approach with $ab$ $initio$ accuracy to address this issue. By training on structural data and corresponding $ab$ $initio$ eigenvalues, the DeePTB model can efficiently predict tight-binding Hamiltonians for unseen structures, enabling efficient simulations of large-size systems under external perturbations such as finite temperatures and strain. This capability is vital for semiconductor band gap engineering and materials design. When combined with molecular dynamics, DeePTB facilitates efficient and accurate finite-temperature simulations of both atomic and electronic behavior simultaneously. This is demonstrated by computing the temperature-dependent electronic properties of a gallium phosphide system with $10^6$ atoms. The availability of DeePTB bridges the gap between accuracy and scalability in electronic simulations, potentially advancing materials science and related fields by enabling large-scale electronic structure calculations.


---
# Determinant- and Derivative-Free Quantum Monte Carlo Within the Stochastic Representation of Wavefunctions

## 波函数的随机表示中的行列式和无导数的量子蒙特卡洛

Link: https://arxiv.org/abs/2402.06577

arXiv:2402.06577v2 Announce Type: replace 
Abstract: Describing the ground states of continuous, real-space quantum many-body systems, like atoms and molecules, is a significant computational challenge with applications throughout the physical sciences. Recent progress was made by variational methods based on machine learning (ML) ansatzes. However, since these approaches are based on energy minimization, ansatzes must be twice differentiable. This (a) precludes the use of many powerful classes of ML models; and (b) makes the enforcement of bosonic, fermionic, and other symmetries costly. Furthermore, (c) the optimization procedure is often unstable unless it is done by imaginary time propagation, which is often impractically expensive in modern ML models with many parameters. The stochastic representation of wavefunctions (SRW), introduced in Nat Commun 14, 3601 (2023), is a recent approach to overcoming (c). SRW enables imaginary time propagation at scale, and makes some headway towards the solution of problem (b), but remains limited by problem (a). Here, we argue that combining SRW with path integral techniques leads to a new formulation that overcomes all three problems simultaneously. As a demonstration, we apply the approach to generalized ``Hooke's atoms'': interacting particles in harmonic wells. We benchmark our results against state-of-the-art data where possible, and use it to investigate the crossover between the Fermi liquid and the Wigner molecule within closed-shell systems. Our results shed new light on the competition between interaction-driven symmetry breaking and kinetic-energy-driven delocalization.


---
# Spin glass model of in-context learning

## 上下文学习的自旋玻璃模型

Link: https://arxiv.org/abs/2408.02288

arXiv:2408.02288v2 Announce Type: replace 
Abstract: Large language models show a surprising in-context learning ability -- being able to use a prompt to form a prediction for a query, yet without additional training, in stark contrast to old-fashioned supervised learning. Providing a mechanistic interpretation and linking the empirical phenomenon to physics are thus challenging and remain unsolved. We study a simple yet expressive transformer with linear attention and map this structure to a spin glass model with real-valued spins, where the couplings and fields explain the intrinsic disorder in data. The spin glass model explains how the weight parameters interact with each other during pre-training, and further clarifies why an unseen function can be predicted by providing only a prompt yet without further training. Our theory reveals that for single-instance learning, increasing the task diversity leads to the emergence of in-context learning, by allowing the Boltzmann distribution to converge to a unique correct solution of weight parameters. Therefore the pre-trained transformer displays a prediction power in a novel prompt setting. The proposed analytically tractable model thus offers a promising avenue for thinking about how to interpret many intriguing but puzzling properties of large language models.


---
# Selecting Relevant Structural Features for Glassy Dynamics by Information Imbalance

## 通过信息不平衡选择玻璃动力学的相关结构特征

Link: https://arxiv.org/abs/2408.12705

arXiv:2408.12705v3 Announce Type: replace 
Abstract: We investigate numerically the identification of relevant structural features that contribute to the dynamical heterogeneity in a model glass-forming liquid. By employing the recently proposed information imbalance technique, we select these features from a range of physically motivated descriptors. This selection process is performed in a supervised manner (using both dynamical and structural data) and an unsupervised manner (using only structural data). We then apply the selected features to predict future dynamics using a machine learning technique. Finally, we discuss the potential applications of this approach in identifying the dominant mechanisms governing the glassy slow dynamics.


---
# A Universal Deep Learning Framework for Materials X-ray Absorption Spectra

## 一种通用的材料x射线吸收谱深度学习框架

Link: https://arxiv.org/abs/2409.19552

arXiv:2409.19552v2 Announce Type: replace 
Abstract: X-ray absorption spectroscopy (XAS) is a powerful characterization technique for probing the local chemical environment of absorbing atoms. However, analyzing XAS data presents significant challenges, often requiring extensive, computationally intensive simulations, as well as significant domain expertise. These limitations hinder the development of fast, robust XAS analysis pipelines that are essential in high-throughput studies and for autonomous experimentation. We address these challenges with OmniXAS, a framework that contains a suite of transfer learning approaches for XAS prediction, each contributing to improved accuracy and efficiency, as demonstrated on K-edge spectra database covering eight 3d transition metals (Ti-Cu). The OmniXAS framework is built upon three distinct strategies. First, we use M3GNet to derive latent representations of the local chemical environment of absorption sites as input for XAS prediction, achieving up to order-of-magnitude improvements over conventional featurization techniques. Second, we employ a hierarchical transfer learning strategy, training a universal multi-task model across elements before fine-tuning for element-specific predictions. Models based on this cascaded approach after element-wise fine-tuning outperform element-specific models by up to 69%. Third, we implement cross-fidelity transfer learning, adapting a universal model to predict spectra generated by simulation of a different fidelity with a higher computational cost. This approach improves prediction accuracy by up to 11% over models trained on the target fidelity alone. Our approach boosts the throughput of XAS modeling by orders of magnitude versus first-principles simulations and is extendable to XAS prediction for a broader range of elements. This transfer learning framework is generalizable to enhance deep-learning models that target other properties in materials research.


---
# Machine learning of quantum channels on NISQ devices

## NISQ设备上量子信道的机器学习

Link: https://arxiv.org/abs/2405.12598

arXiv:2405.12598v2 Announce Type: replace-cross 
Abstract: World-wide efforts aim at the realization of advanced quantum simulators and processors. However, despite the development of intricate hardware and pulse control systems, it may still not be generally known which effective quantum dynamics, or channels, are implemented on these devices. To systematically infer those, we propose a neural-network algorithm approximating generic discrete-time dynamics through the repeated action of an effective quantum channel. We test our approach considering time-periodic Lindblad dynamics as well as non-unitary subsystem dynamics in many-body unitary circuits. Moreover, we exploit it to investigate cross-talk effects on the ibmq_ehningen quantum processor, which showcases our method as a practically applicable tool for inferring quantum channels when the exact nature of the underlying dynamics on the physical device is not known a priori. While the present approach is tailored for learning Markovian dynamics, we discuss how it can be adapted to also capture generic non-Markovian discrete-time evolutions.


---
# Uncertainty Quantification of Fluid Leakage and Fault Instability in Geologic CO2 Storage

## 地质CO2存储中流体泄漏和断层不稳定性的不确定性量化

Link: https://arxiv.org/abs/2411.08039

arXiv:2411.08039v1 Announce Type: new 
Abstract: Geologic CO$_2$ storage is an important strategy for reducing greenhouse gas emissions to the atmosphere and mitigating climate change. In this process, coupling between mechanical deformation and fluid flow in fault zones is a key determinant of fault instability, induced seismicity, and CO$_2$ leakage. Using a recently developed methodology, PREDICT, we obtain probability distributions of the permeability tensor in faults from the stochastic placement of clay smears that accounts for geologic uncertainty. We build a comprehensive set of fault permeability scenarios from PREDICT and investigate the effects of uncertainties from the fault zone internal structure and composition on forecasts of CO$_2$ permanence and fault stability. To tackle the prohibitively expensive computational cost of the large number of simulations required to quantify uncertainty, we develop a deep-learning-based surrogate model capable of predicting flow migration, pressure buildup, and geomechanical responses in CO$_2$ storage operations. We also compare our probabilistic estimation of CO$_2$ leakage and fault instability with previous studies based on deterministic estimates of fault permeability. The results highlight the importance of including uncertainty and anisotropy in modeling of complex fault structures and improved management of geologic CO$_2$ storage projects.


---
# Mobility-based Traffic Forecasting in a Multimodal Transport System

## 多式联运系统中基于移动性的交通预测

Link: https://arxiv.org/abs/2411.08052

arXiv:2411.08052v1 Announce Type: new 
Abstract: We study the analysis of all the movements of the population on the basis of their mobility from one node to another, to observe, measure, and predict the impact of traffic according to this mobility. The frequency of congestion on roads directly or indirectly impacts our economic or social welfare. Our work focuses on exploring some machine learning methods to predict (with a certain probability) traffic in a multimodal transportation network from population mobility data. We analyze the observation of the influence of people's movements on the transportation network and make a likely prediction of congestion on the network based on this observation (historical basis).


---
# MatPilot: an LLM-enabled AI Materials Scientist under the Framework of Human-Machine Collaboration

## MatPilot: 人机协作框架下的LLM AI材料科学家

Link: https://arxiv.org/abs/2411.08063

arXiv:2411.08063v1 Announce Type: new 
Abstract: The rapid evolution of artificial intelligence, particularly large language models, presents unprecedented opportunities for materials science research. We proposed and developed an AI materials scientist named MatPilot, which has shown encouraging abilities in the discovery of new materials. The core strength of MatPilot is its natural language interactive human-machine collaboration, which augments the research capabilities of human scientist teams through a multi-agent system. MatPilot integrates unique cognitive abilities, extensive accumulated experience, and ongoing curiosity of human-beings with the AI agents' capabilities of advanced abstraction, complex knowledge storage and high-dimensional information processing. It could generate scientific hypotheses and experimental schemes, and employ predictive models and optimization algorithms to drive an automated experimental platform for experiments. It turns out that our system demonstrates capabilities for efficient validation, continuous learning, and iterative optimization.


---
# Improved Greedy Identification of Latent Dynamics with Application to Fluid Flows

## 改进的潜在动力学贪婪识别及其在流体流动中的应用

Link: https://arxiv.org/abs/2411.08071

arXiv:2411.08071v1 Announce Type: new 
Abstract: Model reduction is a key technology for large-scale physical systems in science and engineering, as it brings behavior expressed in many degrees of freedom to a more manageable size that subsequently allows control, optimization, and analysis with multi-query algorithms. We introduce an enhanced regression technique tailored to uncover quadratic parametric reduced-order dynamical systems from data. Our method, termed Improved Greedy Identification of Latent Dynamics (I-GILD), refines the learning phase of the original GILD approach. This refinement is achieved by reorganizing the quadratic model coefficients, allowing the minimum-residual problem to be reformulated using the Frobenius norm. Consequently, the optimality conditions lead to a generalized Sylvester equation, which is efficiently solved using the conjugate gradient method. Analysis of the convergence shows that I-GILD achieves superior convergence for quadratic model coefficients compared to GILD's steepest gradient descent, reducing both computational complexity and iteration count. Additionally, we derive an error bound for the model predictions, offering insights into error growth in time and ensuring controlled accuracy as long as the magnitudes of initial error is small and learning residuals are well minimized. The efficacy of I-GILD is demonstrated through its application to numerical and experimental tests, specifically the flow past Ahmed body with a variable rear slant angle, and the lid-driven cylindrical cavity problem with variable Reynolds numbers, utilizing particle-image velocimetry (PIV) data. These tests confirm I-GILD's ability to treat real-world dynamical system challenges and produce effective reduced-order models.


---
# Physics-Informed Neural Networks with Complementary Soft and Hard Constraints for Solving Complex Boundary Navier-Stokes Equations

## 具有互补软硬约束的物理通知神经网络，用于求解复杂边界navier-stokes方程

Link: https://arxiv.org/abs/2411.08122

arXiv:2411.08122v1 Announce Type: new 
Abstract: Soft- and hard-constrained Physics Informed Neural Networks (PINNs) have achieved great success in solving partial differential equations (PDEs). However, these methods still face great challenges when solving the Navier-Stokes equations (NSEs) with complex boundary conditions. To address these challenges, this paper introduces a novel complementary scheme combining soft and hard constraint PINN methods. The soft-constrained part is thus formulated to obtain the preliminary results with a lighter training burden, upon which refined results are then achieved using a more sophisticated hard-constrained mechanism with a primary network and a distance metric network. Specifically, the soft-constrained part focuses on boundary points, while the primary network emphasizes inner domain points, primarily through PDE loss. Additionally, the novel distance metric network is proposed to predict the power function of the distance from a point to the boundaries, which serves as the weighting factor for the first two components. This approach ensures accurate predictions for both boundary and inner domain areas. The effectiveness of the proposed method on the NSEs problem with complex boundary conditions is demonstrated by solving a 2D cylinder wake problem and a 2D blocked cavity flow with a segmented inlet problem, achieving significantly higher accuracy compared to traditional soft- and hard-constrained PINN approaches. Given PINN's inherent advantages in solving the inverse and the large-scale problems, which are challenging for traditional computational fluid dynamics (CFD) methods, this approach holds promise for the inverse design of required flow fields by specifically-designed boundary conditions and the reconstruction of large-scale flow fields by adding a limited number of training input points. The code for our approach will be made publicly available.


---
# Current Progress of Digital Twin Construction Using Medical Imaging

## 利用医学影像构建数字孪生体的最新进展

Link: https://arxiv.org/abs/2411.08173

arXiv:2411.08173v1 Announce Type: new 
Abstract: Medical imaging has played a pivotal role in advancing and refining digital twin technology, allowing for the development of highly personalized virtual models that represent human anatomy and physiological functions. A key component in constructing these digital twins is the integration of high-resolution imaging data, such as MRI, CT, PET, and ultrasound, with sophisticated computational models. Advances in medical imaging significantly enhance real-time simulation, predictive modeling, and early disease diagnosis, individualized treatment planning, ultimately boosting precision and personalized care. Although challenges persist, such as the complexity of anatomical modeling, integrating various imaging modalities, and high computational demands, recent progress in imaging and machine learning has greatly improved the precision and clinical applicability of digital twins. This review investigates the role of medical imaging in developing digital twins across organ systems. Key findings demonstrate that improvements in medical imaging have enhanced the diagnostic and therapeutic potential of digital twins beyond traditional methods, particularly in imaging accuracy, treatment effectiveness, and patient outcomes. The review also examines the technical barriers that currently limit further development of digital twin technology, despite advances in medical imaging, and outlines future research avenues aimed at overcoming these challenges to unlock the full potential of this technology in precision medicine.


---
# Developing a Foundation Model for Predicting Material Failure

## 开发用于预测材料失效的基础模型

Link: https://arxiv.org/abs/2411.08354

arXiv:2411.08354v1 Announce Type: new 
Abstract: Understanding material failure is critical for designing stronger and lighter structures by identifying weaknesses that could be mitigated. Existing full-physics numerical simulation techniques involve trade-offs between speed, accuracy, and the ability to handle complex features like varying boundary conditions, grid types, resolution, and physical models. We present the first foundation model specifically designed for predicting material failure, leveraging large-scale datasets and a high parameter count (up to 3B) to significantly improve the accuracy of failure predictions. In addition, a large language model provides rich context embeddings, enabling our model to make predictions across a diverse range of conditions. Unlike traditional machine learning models, which are often tailored to specific systems or limited to narrow simulation conditions, our foundation model is designed to generalize across different materials and simulators. This flexibility enables the model to handle a range of material properties and conditions, providing accurate predictions without the need for retraining or adjustments for each specific case. Our model is capable of accommodating diverse input formats, such as images and varying simulation conditions, and producing a range of outputs, from simulation results to effective properties. It supports both Cartesian and unstructured grids, with design choices that allow for seamless updates and extensions as new data and requirements emerge. Our results show that increasing the scale of the model leads to significant performance gains (loss scales as $N^{-1.6}$, compared to language models which often scale as $N^{-0.5}$).


---
# Scaling Function Learning: A sparse aerodynamic data reconstruction method for generalizing aircraft shapes

## 缩放函数学习: 一种用于飞机形状泛化的稀疏空气动力学数据重建方法

Link: https://arxiv.org/abs/2411.08662

arXiv:2411.08662v1 Announce Type: new 
Abstract: Accurate and complete aerodynamic data sets are the basis for comprehensive and accurate evaluation of the overall performance of aircraft. However, the sampling cost of full-state aerodynamic data is extremely high, and there are often differences between wind tunnel conditions and actual flight conditions. Conventional scaling parameter extraction methods can solve the problem of aerodynamic state extrapolation, but hard to achieve data migration and shape generalization. In order to realize the low-cost construction of a full-state nonlinear aerodynamic database, this research proposes the Scaling Function Learning (SFL) method. In SFL method, symbolic regression is used to mine the composite function expression of aerodynamic force coefficient for a relatively complete aerodynamic data set of typical aircraft. The inner layer of the function represents a scaling function. The SFL method was validated on the HB-2 by extracting scaling parameters for axial force coefficients and generalizing the scaling function by releasing its constants. The effectiveness and accuracy of the scaling function are verified using different hypersonic aircraft configurations, such as HBS, double ellipsoid, sharp cone, and double cone missile. The results show that the extracted scaling function has the ability to generalize across states and configurations. With only 3-4 state samples, the aerodynamic database construction of variable Mach number, angle of attack and Reynolds number can be realized, which shows great state extrapolation ability with a relative error of about 1-5%. This research also lays a methodological foundation for parameter space dimensionality reduction and small sample modeling of other complex high-dimensional engineering problems.


---
# Physics-Informed Transformation Toward Improving the Machine-Learned NLTE Models of ICF Simulations

## 向改进ICF仿真的机器学习NLTE模型的物理信息转换

Link: https://arxiv.org/abs/2411.08789

arXiv:2411.08789v1 Announce Type: new 
Abstract: The integration of machine learning techniques into Inertial Confinement Fusion (ICF) simulations has emerged as a powerful approach for enhancing computational efficiency. By replacing the costly Non-Local Thermodynamic Equilibrium (NLTE) model with machine learning models, significant reductions in calculation time have been achieved. However, determining how to optimize machine learning-based NLTE models in order to match ICF simulation dynamics remains challenging, underscoring the need for physically relevant error metrics and strategies to enhance model accuracy with respect to these metrics. Thus, we propose novel physics-informed transformations designed to emphasize energy transport, use these transformations to establish new error metrics, and demonstrate that they yield smaller errors within reduced principal component spaces compared to conventional transformations.


---
# Machine Learning-Based Enhancements of Empirical Energy Functions: Structure, Dynamics and Spectroscopy of Modified Benzenes

## 基于机器学习的经验能量函数的增强: 改性苯的结构，动力学和光谱

Link: https://arxiv.org/abs/2411.08831

arXiv:2411.08831v1 Announce Type: new 
Abstract: The effect of replacing individual contributions to an empirical energy function are assessed for halogenated benzenes (X-Bz, X = H, F, Cl, Br) and chlorinated phenols (Cl-PhOH). Introducing electrostatic models based on distributed charges (MDCM) instead of usual atom-centered point charges yields overestimated hydration free energies unless the van der Waals parameters are reparametrized. Scaling van der Waals ranges by 10 \% to 20 \% for three Cl-PhOH and most X-Bz yield results within experimental error bars, which is encouraging, whereas for benzene (H-Bz) point charge-based models are sufficient. Replacing the bonded terms by a neural network-trained energy function with either fluctuating charges or MDCM electrostatics also yields qualitatively correct hydration free energies which still require adaptation of the van der Waals parameters. The infrared spectroscopy of Cl-PhOH is rather well predicted by all models although the ML-based energy function performs somewhat better in the region of the framework modes. It is concluded that refinements of empirical energy functions for targeted applications is a meaningful way towards more quantitative simulations.


---
# Data-driven Surface Solar Irradiance Estimation using Neural Operators at Global Scale

## 全球范围内使用神经算子的数据驱动的表面太阳辐照度估计

Link: https://arxiv.org/abs/2411.08843

arXiv:2411.08843v1 Announce Type: new 
Abstract: Accurate surface solar irradiance (SSI) forecasting is essential for optimizing renewable energy systems, particularly in the context of long-term energy planning on a global scale. This paper presents a pioneering approach to solar radiation forecasting that leverages recent advancements in numerical weather prediction (NWP) and data-driven machine learning weather models. These advances facilitate long, stable rollouts and enable large ensemble forecasts, enhancing the reliability of predictions. Our flexible model utilizes variables forecast by these NWP and AI weather models to estimate 6-hourly SSI at global scale. Developed using NVIDIA Modulus, our model represents the first adaptive global framework capable of providing long-term SSI forecasts. Furthermore, it can be fine-tuned using satellite data, which significantly enhances its performance in the fine-tuned regions, while maintaining accuracy elsewhere. The improved accuracy of these forecasts has substantial implications for the integration of solar energy into power grids, enabling more efficient energy management and contributing to the global transition to renewable energy sources.


---
# Energy Dissipation Preserving Physics Informed Neural Network for Allen-Cahn Equations

## 用于allen-cahn方程的能量耗散保持物理通知神经网络

Link: https://arxiv.org/abs/2411.08760

arXiv:2411.08760v1 Announce Type: cross 
Abstract: This paper investigates a numerical solution of Allen-Cahn equation with constant and degenerate mobility, with polynomial and logarithmic energy functionals, with deterministic and random initial functions, and with advective term in one, two, and three spatial dimensions, based on the physics-informed neural network (PINN). To improve the learning capacity of the PINN, we incorporate the energy dissipation property of the Allen-Cahn equation as a penalty term into the loss function of the network. To facilitate the learning process of random initials, we employ a continuous analogue of the initial random condition by utilizing the Fourier series expansion. Adaptive methods from traditional numerical analysis are also integrated to enhance the effectiveness of the proposed PINN. Numerical results indicate a consistent decrease in the discrete energy, while also revealing phenomena such as phase separation and metastability.


---
# Flow reconstruction in time-varying geometries using graph neural networks

## 使用图神经网络在时变几何中进行流重建

Link: https://arxiv.org/abs/2411.08764

arXiv:2411.08764v1 Announce Type: cross 
Abstract: The paper presents a Graph Attention Convolutional Network (GACN) for flow reconstruction from very sparse data in time-varying geometries. The model incorporates a feature propagation algorithm as a preprocessing step to handle extremely sparse inputs, leveraging information from neighboring nodes to initialize missing features. In addition, a binary indicator is introduced as a validity mask to distinguish between the original and propagated data points, enabling more effective learning from sparse inputs. Trained on a unique data set of Direct Numerical Simulations (DNS) of a motored engine at a technically relevant operating condition, the GACN shows robust performance across different resolutions and domain sizes and can effectively handle unstructured data and variable input sizes. The model is tested on previously unseen DNS data as well as on an experimental data set from Particle Image Velocimetry (PIV) measurements that were not considered during training. A comparative analysis shows that the GACN consistently outperforms both a conventional Convolutional Neural Network (CNN) and cubic interpolation methods on the DNS and PIV test sets by achieving lower reconstruction errors and better capturing fine-scale turbulent structures. In particular, the GACN effectively reconstructs flow fields from domains up to 14 times larger than those observed during training, with the performance advantage increasing for larger domains.


---
# The effect of dataset size and the process of big data mining for investigating solar-thermal desalination by using machine learning

## 数据集大小和大数据挖掘过程对利用机器学习研究太阳能-热海水淡化的影响

Link: https://arxiv.org/abs/2307.12594

arXiv:2307.12594v2 Announce Type: replace 
Abstract: Machine learning's application in solar-thermal desalination is limited by data shortage and inconsistent analysis. This study develops an optimized dataset collection and analysis process for the representative solar still. By ultra-hydrophilic treatment on the condensation cover, the dataset collection process reduces the collection time by 83.3%. Over 1,000 datasets are collected, which is nearly one order of magnitude larger than up-to-date works. Then, a new interdisciplinary process flow is proposed. Some meaningful results are obtained that were not addressed by previous studies. It is found that Radom Forest might be a better choice for datasets larger than 1,000 due to both high accuracy and fast speed. Besides, the dataset range affects the quantified importance (weighted value) of factors significantly, with up to a 115% increment. Moreover, the results show that machine learning has a high accuracy on the extrapolation prediction of productivity, where the minimum mean relative prediction error is just around 4%. The results of this work not only show the necessity of the dataset characteristics' effect but also provide a standard process for studying solar-thermal desalination by machine learning, which would pave the way for interdisciplinary study.


---
# A Review of Electromagnetic Elimination Methods for low-field portable MRI scanner

## 低场便携式MRI扫描仪的电磁消除方法综述

Link: https://arxiv.org/abs/2406.17804

arXiv:2406.17804v3 Announce Type: replace 
Abstract: This paper analyzes conventional and deep learning methods for eliminating electromagnetic interference (EMI) in MRI systems. We compare traditional analytical and adaptive techniques with advanced deep learning approaches. Key strengths and limitations of each method are highlighted. Recent advancements in active EMI elimination, such as external EMI receiver coils, are discussed alongside deep learning methods, which show superior EMI suppression by leveraging neural networks trained on MRI data. While deep learning improves EMI elimination and diagnostic capabilities, it introduces security and safety concerns, particularly in commercial applications. A balanced approach, integrating conventional reliability with deep learning's advanced capabilities, is proposed for more effective EMI suppression in MRI systems.


---
# Machine learning models with different cheminformatics data sets to forecast the power conversion efficiency of organic solar cells

## 使用不同化学信息学数据集的机器学习模型预测有机太阳能电池的功率转换效率

Link: https://arxiv.org/abs/2410.23444

arXiv:2410.23444v2 Announce Type: replace 
Abstract: Random Forest (RF) and Gradient Boosting Regression Trees (GBRT) regression models along with three cheminformatics data sets (RDkit, Mordred, Morgan) have been used to predict the power conversion efficiency (PCE) of organic solar cells (OSCs). The data consists of cheinformatics descriptors of the electron donor used in 433 OSCs for which the experimental PCE (target variable) is reported in the literature. The donor is either a polymer or a small organic molecule, and the acceptor the fullerene derivatives PCBM or PC71BM. Unlike previous methods, our ML approach considers the type of donor and the acceptor by adding four extra donor's features using the one-hot encoder tool. It is demonstrated that this additional information improves the prediction performance up to 34%. We have also exploited this feature to theoretically forecast the PCE of new OSCs by evaluating the ML model for a different acceptor. It is predicted that more than 50% of the OCSs obtained by exchanging the acceptor would have higher experimental PCE. The prediction accuracy of a given ML approach is analyzed for different PCE intervals. RF using RDkit descriptors resulted in the best ML approach with a Pearson's correlation coefficient for the training and testing sets equal to 0.96 and 0.62, respectively.


---
# Accurate Unsupervised Photon Counting from Transition Edge Sensor Signals

## 来自过渡边缘传感器信号的精确无监督光子计数

Link: https://arxiv.org/abs/2411.05737

arXiv:2411.05737v2 Announce Type: replace 
Abstract: We compare methods for signal classification applied to voltage traces from transition edge sensors (TES) which are photon-number resolving detectors fundamental for accessing quantum advantages in information processing, communication and metrology. We quantify the impact of numerical analysis on the distinction of such signals. Furthermore, we explore dimensionality reduction techniques to create interpretable and precise photon number embeddings. We demonstrate that the preservation of local data structures of some nonlinear methods is an accurate way to achieve unsupervised classification of TES traces. We do so by considering a confidence metric that quantifies the overlap of the photon number clusters inside a latent space. Furthermore, we demonstrate that for our dataset previous methods such as the signal's area and principal component analysis can resolve up to 16 photons with confidence above $90\%$ while nonlinear techniques can resolve up to 21 with the same confidence threshold. Also, we showcase implementations of neural networks to leverage information within local structures, aiming to increase confidence in assigning photon numbers. Finally, we demonstrate the advantage of some nonlinear methods to detect and remove outlier signals.


---
# Deep learning tight-binding approach for large-scale electronic simulations at finite temperatures with $ab$ $initio$ accuracy

## 有限温度下大规模电子模拟的深度学习紧绑定方法，具有 $ ab $ initio $ 精度

Link: https://arxiv.org/abs/2307.04638

arXiv:2307.04638v3 Announce Type: replace-cross 
Abstract: Simulating electronic behavior in materials and devices with realistic large system sizes remains a formidable task within the $ab$ $initio$ framework due to its computational intensity. Here we show DeePTB, an efficient deep learning-based tight-binding approach with $ab$ $initio$ accuracy to address this issue. By training on structural data and corresponding $ab$ $initio$ eigenvalues, the DeePTB model can efficiently predict tight-binding Hamiltonians for unseen structures, enabling efficient simulations of large-size systems under external perturbations such as finite temperatures and strain. This capability is vital for semiconductor band gap engineering and materials design. When combined with molecular dynamics, DeePTB facilitates efficient and accurate finite-temperature simulations of both atomic and electronic behavior simultaneously. This is demonstrated by computing the temperature-dependent electronic properties of a gallium phosphide system with $10^6$ atoms. The availability of DeePTB bridges the gap between accuracy and scalability in electronic simulations, potentially advancing materials science and related fields by enabling large-scale electronic structure calculations.


---
# Probabilistic Emulation of a Global Climate Model with Spherical DYffusion

## 具有球形DYffusion的全球气候模型的概率仿真

Link: https://arxiv.org/abs/2406.14798

arXiv:2406.14798v2 Announce Type: replace-cross 
Abstract: Data-driven deep learning models are transforming global weather forecasting. It is an open question if this success can extend to climate modeling, where the complexity of the data and long inference rollouts pose significant challenges. Here, we present the first conditional generative model that produces accurate and physically consistent global climate ensemble simulations by emulating a coarse version of the United States' primary operational global forecast model, FV3GFS. Our model integrates the dynamics-informed diffusion framework (DYffusion) with the Spherical Fourier Neural Operator (SFNO) architecture, enabling stable 100-year simulations at 6-hourly timesteps while maintaining low computational overhead compared to single-step deterministic baselines. The model achieves near gold-standard performance for climate model emulation, outperforming existing approaches and demonstrating promising ensemble skill. This work represents a significant advance towards efficient, data-driven climate simulations that can enhance our understanding of the climate system and inform adaptation strategies.


---
# Circuit design in biology and machine learning. I. Random networks and dimensional reduction

## 生物学和机器学习中的电路设计。I.随机网络与降维

Link: https://arxiv.org/abs/2408.09604

arXiv:2408.09604v2 Announce Type: replace-cross 
Abstract: A biological circuit is a neural or biochemical cascade, taking inputs and producing outputs. How have biological circuits learned to solve environmental challenges over the history of life? The answer certainly follows Dobzhansky's famous quote that ``nothing in biology makes sense except in the light of evolution.'' But that quote leaves out the mechanistic basis by which natural selection's trial-and-error learning happens, which is exactly what we have to understand. How does the learning process that designs biological circuits actually work? How much insight can we gain about the form and function of biological circuits by studying the processes that have made those circuits? Because life's circuits must often solve the same problems as those faced by machine learning, such as environmental tracking, homeostatic control, dimensional reduction, or classification, we can begin by considering how machine learning designs computational circuits to solve problems. We can then ask: How much insight do those computational circuits provide about the design of biological circuits? How much does biology differ from computers in the particular circuit designs that it uses to solve problems? This article steps through two classic machine learning models to set the foundation for analyzing broad questions about the design of biological circuits. One insight is the surprising power of randomly connected networks. Another is the central role of internal models of the environment embedded within biological circuits, illustrated by a model of dimensional reduction and trend prediction. Overall, many challenges in biology have machine learning analogs, suggesting hypotheses about how biology's circuits are designed.


---
# Pairing a Global Optimization Algorithm with EXAFS to Accelerate Prediction of Lanthanide Structures in Solution

## 将全局优化算法与EXAFS配对以加速溶液中镧系元素结构的预测

Link: https://dx.doi.org/10.26434/chemrxiv-2024-lgkjh-v3?rft_dat=source%3Ddrss

Ensemble-average sampling of structures sampled from ab initio molecular dynamics (AIMD) simulations can be used to predict theoretical extended X-ray absorption fine structure (EXAFS) signals that closely match experimental spectra. However, AIMD simulations are time-consuming and resource-intensive, particularly for solvated lanthanide ions, which often form multiple non-rigid geometries with high coordination numbers. To accelerate the characterization of lanthanide structures in solution, we employed the Northwest Potential Energy Surface Search Engine (NWPEsSe), an adaptive-learning global optimization algorithm, to efficiently screen first-shell structures. As case studies, we examine two systems: Eu(NO3)3 dissolved in acetonitrile with a terpyridine ligand (terpyNO2), and Nd(NO3)3 dissolved in acetonitrile. The theoretical spectra for structures identified by NWPEsSe were compared to both experimental and AIMD-derived EXAFS spectra. The NWPEsSe algorithm successfully identified the proper solvation structure for both Eu(NO3)3(terpyNO2) and Nd(NO3)(acetonitrile)3, with the calculated EXAFS signals closely matching the experimental spectra for the Eu-ligand complex and showing good similarity for the Nd salt; the better agreement with the ligand-containing structure is attributed to a less dynamic coordination environment due to the rigid ligand. The key advantage of the global optimization algorithm lies in its ability to sample the coordination environment across the potential energy surface. The structure identification process can be done with a variety of methods and generally has reduced the time required from a month to within a week.


---
# Analysing the Influence of Infrastructure and Power Control on Cellular and Cell-Free Massive MIMO Systems: Insights from Machine Learning

## 分析基础设施和功率控制对蜂窝和无蜂窝大规模MIMO系统的影响: 来自机器学习的见解

Link: https://www.researchsquare.com/article/rs-5206122/latest

In large-scale multiple-input-multiple-output (mMIMO) networks, effective power control (PC) mechanisms play a pivotal role. Various algorithms, such as the weighted mean square error (WMMSE) algorithm, are utilized to estimate PCs, demanding considerable computational resources. This study examines the performance of PC in mMIMO systems, emphasizing the aggregate spectral efficiency (sum SE) and the cumulative distribution function (CDF) constrained by SE per user equipment (UE). This investigation explores the impact of different factors, including the number of UEs, access points/base stations (APs/BSs), and the implementation of deep neural network (DNN)-based PCs, within both cellular (CL) and cell-free (CF) architectures. Through empirical analysis, the study elucidates the influence of parameter 'g' on the DNN versus WMMSE comparison curve, underscoring the importance of accounting for the quantity of APs/BSs and antennas to attain optimal PC performance.


---
# DT-GCNN: Dynamic Triplet Network with GRU-CNN for Enhanced Text Classification

## Dt-gcnn: 具有gru-cnn的动态三元组网络，用于增强文本分类

Link: https://www.researchsquare.com/article/rs-5359853/latest

Text classification is a crucial task in natural language processing, and deep learning models have demonstrated exceptional performance in this domain. However, many deep neural network models struggle to handle complex and imbalanced datasets, and their adaptability across multiple scenarios remains limited. To tackle these challenges, this paper proposes DT-GCNN, a model that integrates GRU for capturing sequence information and CNN for extracting local features. Furthermore, the model incorporates an adaptive soft-margin triplet loss function that dynamically adjusts triplet margins, thereby enhancing the learning quality of the embedding space. During the training process, intelligent algorithms are periodically employed to dynamically reconstruct triplets, thereby enhancing the model's generalization capability. This study conducts extensive experiments on eight datasets from various categories. The results demonstrate that DT-GCNN outperforms most existing baseline models, showing notable superiority in handling complex and imbalanced category tasks. The proposed method also significantly enhances generalization ability and stability, exhibiting excellent performance across diverse datasets.


---
# Research on scheduling rule extraction and model comparison based on autoencoder and self-attention mechanism

## 基于自编码器和自注意机制的调度规则提取及模型比较研究

Link: https://www.researchsquare.com/article/rs-5345488/latest

Reservoir operation rule function is a functional mapping method that reflects specific operational rules to guide reservoir operations. With the deepening of research, the scheduling function has changed from linear to nonlinear forms, from single-objective functions to integrated multi-objective functions, from ignoring uncertainty to incorporating uncertainty analysis, from using fixed parameters to parameters that dynamically change over time, and the system has been continuously refined. Different dispatching rule functions have different actual dispatching effects (flood control, power generation benefits, water resource utilization rate and reliability). Reservoir optimal dispatching aims to identify the best rule functions to achieve the specified objectives. Through advanced computing technologies, the complex relationships between dispatch-related factors and decision variables can be extracted from large amounts of historical reservoir operation data, thus facilitating the extraction of reservoir dispatch rules. With the construction of more and more reservoirs, the hydraulic connections between reservoir groups are complex, with mutual influences occurring across both spatial and temporal scales. These complex correlation factors need to be considered when extracting dispatch rule functions. Compared to conventional operation charts, the operation rule functions extracted through data mining methods offer greater convenience in reservoir group operations. This study adopts a machine learning model based on an autoencoder and self-attention mechanism, focusing on key reservoirs along the Yangtze River and its tributaries. A combined model of these techniques is proposed, and the natural gradient boosting scheduling comparison model is applied to optimize and compare the extracted scheduling rules. The applicability of the model is verified through analysis, and the results are further explained using SHAP (SHapley Additive exPlanations) theory, which provides interpretability for the model results.


---
# Knowledge Distillation with Applications to Interpretable Arabic Sentiment Analysis

## 知识蒸馏及其在可解释阿拉伯语情感分析中的应用

Link: https://www.researchsquare.com/article/rs-5356825/latest

Sentiment analysis stands as a focal point in the current landscape of natural language processing research with deep neural network models as being prevalent tools of choice. While these models have exhibited noteworthy performance, their intricate nature frequently renders them akin to black boxes, resulting in a lack of transparency regarding the internal mechanisms of the sentiment classification process. The lack of interpretability in such models raises concerns regarding the reliance on outcomes from opaque systems. This study introduces an approach for distilling knowledge from complex deep neural network models into simpler and more interpretable ones while maintaining performance and ensuring global interpretability. Three distinct knowledge distillation pipelines are proposed to transfer the knowledge acquired by teacher models, including Long Short-Term Memory, Bidirectional Long Short-Term Memory, Convolutional Neural Network and AraBERT into Logistic Regression and Decision Tree models. Conducting thorough assessments across three separate datasets for Arabic sentiment analysis, the study&amp;rsquo;s proposed approach consistently demonstrates performance levels that surpass those of complex models.


---
# Comparative Analysis of Linear Regression, Decision Tree, Xgboost, Catboost, and Artificial Neural Network Machine Learning Algorithms for

## 比较分析线性回归、决策树、Xgboost、Catboost和人工神经网络机器学习算法，用于

Link: https://www.researchsquare.com/article/rs-5442566/latest

The compressive strength of concrete was predicted using various machine learning algorithms: Linear Regression (LR), Decision Tree (DT), Xgboost, Catboost, and Artificial Neural Network (ANN), to determine the best performant for concrete compressive strength prediction. From the analysis report, the best-performing model was the Catboost model, followed by the Xgboost model, with mean absolute errors of 2.72 N/mm2 and 3 N/mm2 respectively. The least performing models for concrete prediction from the research are the LR and the ANN with very high mean absolute errors of 7.75 N/mm2 and 4.9 N/mm2 respectively. The dataset from which the models were built, and insights drawn was obtained from kaggle.com.


---
# An Instructional Emperor Pigeon Optimization (IEPO) based DeepEnrollNet for Student Enrolment Prediction and Retention Recommendation at Majmaah University

## 基于教学皇帝鸽子优化 (IEPO) 的DeepEnrollNet，用于Majmaah大学的学生入学预测和保留建议

Link: https://www.researchsquare.com/article/rs-5176085/latest

Academic institutions increasingly require the ability to manage enrollment and track student retention, comprehensively plan their future course list based on campus demand to better support students in a broader variety of disciplines. In this study, we presented an innovative framework to predict student enrollment and prevent students from churn in Majmaah University by using deep learning-based approach on residuos analysis combined with recommender system to deal both text and numeric data. The proposed framework includes advanced preprocessing techniques such as K-Nearest Neighbors (KNN) imputation and Z-score normalization for numeric data, complemented by text processing methods including stop word removal, stemming, lemmatization, tokenization, and Named Entity Recognition (NER). Feature extraction is performed using statistical measures (mean, median, standard deviation, skewness, kurtosis) for numeric data, and advanced techniques such as word embeddings (GloVe), topic modeling (Latent Dirichlet Allocation - LDA), and sentiment analysis (SentiWordNet) for text data. A weighted feature fusion approach integrates these features. The optimal features are selected using the Pythagorean fuzzy AHP with Hybrid Optimization approach (Instructional Emperor Pigeon Optimization (IEPO)). The DeepEnrollNet model, employing CNN-GRU-Attention QCNN, is utilized for accurate enrollment prediction, while Deep Q-Networks (DQN) are applied to generate actionable retention recommendations. This comprehensive methodology aims to enhance predictive accuracy and develop effective strategies for improving student retention at Majmaah University.


---
# Analyzing the Impact of Symbols in Taiwan&rsquo;s Anti-Disinformation Campaign on TikTok during Elections

## 试析台湾反造形运动中的符号对选举期间TikTok的影响

Link: https://www.researchsquare.com/article/rs-5182951/latest

Social, cultural, and political (SCP) symbols in campaigns play vital roles in connecting emotionally with people, representing identity, simplifying complex issues, and reinforcing political ideologies. This study investigates the impact of SCP symbol content on engagement, emotional responses, and trust in electoral process in TikTok-based anti-disinformation campaigns during Taiwanese 2024 general elections. Utilizing advanced natural language processing and machine learning, we analyzed posts and comments, categorizing them for presence and type of SCP symbols. We employed a multi-faceted approach, examining engagement metrics, emotional responses, and trust evaluations across different symbol categories. Our novel categorization ranged from content without symbols to those incorporating multiple symbol types, providing nuanced understanding of symbolic influence. Findings reveal a strong positive correlation between the complexity of symbolic content and user engagement, with cultural symbols emerging as potent in driving interaction and fostering trust. Emotional analysis showed symbol-rich content elicited more positive emotional responses, especially with cultural symbols. Trust evaluations showed increases in symbol diversity increased trust, with cultural and political symbols generating the highest trust levels towards Taiwan&rsquo;s democratic and electoral process. Statistical validation through non-parametric tests confirmed that the observed effects were not artifacts of data distribution or follower count variations. Our findings have significant implications for public affairs, strategic communications, policymakers, and platform managers, providing a framework for crafting more 1 effective, culturally resonant content in digital socio-political campaigns. This study also opens avenues for further research into the long-term effects of symbolic communication on political behavior and democratic participation in the digital age.


---
# Blockchain enabled Deep Learning Architectures to Secure IoT and Edge Computing in Supply Chain Management for Industry 4.0

## 区块链支持深度学习架构，以确保行业4.0供应链管理中的物联网和边缘计算

Link: https://www.researchsquare.com/article/rs-5375173/latest

Blockchain has been widely used in Internet of Things (IoT) applications to ensure data confidentiality, consistency and traceability. Integration of blockchain and edge computing improves the resource utilization across network, computation, storage, and security. This paper presents a novel method to address data transmission, communication overhead, security and privacy along with accuracy in IoT application integrating cloud edge environments using deep learning techniques. In addition, Blockchain technology has been incorporated to avoid the cyber-attacks in supply chain management and secure data transmission has been enhanced using Voxel Convolutional networks. The experiments are conducted to evaluate and analyse data transmission ratio, specificity, training accuracy, and validation accuracy. The computational results of our approach show a significant improvement in data privacy and communication security with data transmission ratio up to 89%, validation accuracy 89%, specificity of 75%, training accuracy 95%, and security analysis 91%.


---
# Predicting Fire Incidents with ML&nbsp;: an XAI approach

## 用ML预测火灾事故: 一种XAI方法

Link: https://www.researchsquare.com/article/rs-5356484/latest

With increasing urbanization and industrial activities in Dublin city, there is a growing interest in natural disasters and accidents. In this research, we compared the regression performances for the number of fire incidents in Dublin with seven different machine learning models. We evaluated how each model performs with metrics like Mean Squared Error (MSE), Mean Absolute Error (MAE), Mean Absolute Percentage Error (MAPE), and R-Squared scores. Among the used models which are Prophet, Auto-Regressive Integrated Moving Average (ARIMA), Simple Linear Regression, Polynomial Regression, Support Vector Regression, Decision Tree Regression, and Random Forest Regression, the most precise model was the Prophet with the highest R-Squared score of 0.91 because it effectively captures the underlying trend, seasonality and holiday effects. This research also aims not only to establish the superior model but also to give clear and understandable reasons for these predictions using explainable AI (XAI). In particular, the trust in Prophet is enhanced by global explanation and local explanation for users to believe in the decision-making processes in the model. It enabled us to enhance the interpretability and transparency of the Prophet, which is aligning with ethical AI (Artificial Intelligence).


---
# Reinforcement Learning-Based Modulation Strategy for Dual Active Bridge Converter

## 基于强化学习的双有源桥变换器调制策略

Link: https://www.researchsquare.com/article/rs-5375979/latest

With the continuous growth of global energy demand and the depletion of non-renewable resources, the role of power electronic converters in energy conversion and distribution has become increasingly significant. This paper addresses the modulation strategy for the Dual Active Bridge (DAB) converter under complex operating conditions, proposing an optimization method based on reinforcement learning (RL). By incorporating Deep Q-Network (DQN) and Double Deep Q-Network (DDQN) algorithms, an intelligent agent control system is designed. The agent learns and optimizes the phase-shift modulation strategy through interactions with the DAB circuit environment, aiming to achieve high efficiency and stability in power transmission. Simulations conducted using MATLAB demonstrate that the RL-based DAB modulation strategy significantly enhances stability and self-stability compared to traditional control strategies across various operating conditions. The proposed RL-based control strategy for DAB converters offers a novel approach to overcoming the limitations of conventional control methods in complex scenarios. It not only enhances the performance of DAB converters but also provides new research perspectives and theoretical foundations for the intelligent control of power electronic converters.


---
# Improving Online Security: A Deep Learning Model for Phishing URL Detection

## 提高在线安全性: 用于网络钓鱼URL检测的深度学习模型

Link: https://www.researchsquare.com/article/rs-5363511/latest

Today, phishing attacks have become a sig-nificant threat to online security. These false fronts areused to mislead people and businesses into clicking onmalicious URLs, revealing their password, credit cardinformation, or personal data. Infiltrating fraudulentwebsites or fake apps: Cybercriminals are upping theirgame by creating more and more representative de-signs of what to the inexperienced user seems like a&ldquo;trustworthy&rdquo; site, where they can be misled to entertheir username and password. Various methods such asblacklist, whitelist, heuristic exchange features, and vi-sual similarity-based techniques are being proposed foranti-phishing. Modern browsers come to help and limitthe way users can be phished into a vicious agenda,but yet, users fall prey to these obvious attacks and re-veal their secret data. In earlier research, the authorsintroduced a machine-learning approach for detectingphishing websites, achieving an accuracy of 96.85% with87 features. In this paper, we present new phishingURL detection models based on a Deep Neural Net-work (DNN) using the same dataset and 87 featuresfrom the previous work. The proposed method achievesa higher accuracy of 99.43% with DNN. Additionally,cross-validation is employed to enhance robustness andimprove the speed of phishing detection


---
# An Intelligent Duty Cycle Forecasting and Optimized Clustering Algorithm for Improving Energy Efficiency in Multi-hop WSNs

## 一种提高多跳无线传感器网络能效的智能占空比预测和优化分簇算法

Link: https://www.researchsquare.com/article/rs-5355720/latest

Wireless Sensor Networks (WSNs) rely on clustering for energy-efficient routing. This involves dividing networks into clusters and optimizing routing paths based on energy and distance. Various clustering routing algorithms have been developed, with the Sine Cosine method and L&amp;eacute;vy mutation (SCA-L&amp;eacute;vy) showing superior energy efficiency and network lifespan. However, this method can lead to Quality-of-Service (QoS) issues, such as increased delay in intra- and inter-cluster transmission as network capacity grows, and transmission range limitations in multi-hop WSNs. This results in an ineffective tradeoff between energy usage and delay. Therefore, this paper introduces the Intelligent Duty Cycle adapted SCA-L&amp;eacute;vy Clustering (IDCSC) based routing algorithm for multi-hop WSN. At first, the SCA-L&amp;eacute;vy algorithm is applied during the setup phase to create the WSN clusters and choose the optimal Cluster Head (CH) in each cluster based on the node&amp;rsquo;s residual energy and distance. Then, during the data transmission phase, a joint inter- and intra-cluster energy reduction strategy is proposed to select the multi-hop path for transmitting data from nodes to the Base Station (BS). For intra-cluster communication, this strategy involves implementing a Forecast-based Duty-Cycle Adaptation (FDCA) using the Recurrent Neural Network (RNN) model to minimize energy consumption based on the distance between CH and child nodes. For inter-cluster communication, the path with the lowest energy consumption is selected, resulting in low energy dissipation and delay in multi-hop WSNs. Finally, extensive simulations demonstrate that the IDCSC algorithm attains a greater QoS efficiency in contrast with the conventional clustering routing algorithms.


---
# Are LLMs capable of understanding sarcastic conversations? MuKX: architecture for understanding sarcasm

## LLMs能够理解讽刺对话吗？MuKX: 理解讽刺的架构

Link: https://www.researchsquare.com/article/rs-5340698/latest

The widespread adoption of Large Language Models (LLMs) for understanding natural language makes us believe to be the de-facto standard for natural language processing tasks. In practice, natural language conversations are not limited to monolingual conversations. A significant number of internet users are multi-lingual. Such multi-lingual userbase has led to exponential growth in code-mix text content. In this study, inclusive of monolingual we evaluate the true capabilities and weaknesses of LLMs for understanding sarcastic code-mix conversations on the internet. Considering the morphological linguistic features of sarcastic instance, we have proposed an Ensemble-learning inspired novel architecture, MuKX. MuKX outperforms the state-of-the-art code-mix by 12.68% and monolingual by 8.42%. MuKX is a continuous effort, and we will expand it with more low-resource languages and tasks.


---
# &ldquo;It is exhausting &hellip; to keep up an interested and pleasant eager face throughout&rdquo;: Impression management in the clinical learning environment

## “在整个过程中保持一个感兴趣和愉快的渴望的面孔”: 临床学习环境中的印象管理

Link: https://www.researchsquare.com/article/rs-5379896/latest

Background: Existing literature suggests that learning during clinical placements is predominantly informal and unstructured, requiring medical students to be proactive and agentic to maximise learning opportunities. Exploring ways in which students navigate social structures of the clinical learning environment (CLE) through Goffman&amp;rsquo;s theory of impression management should illuminate our perspective on agentic efforts related to work-based learning. This in-turn should aid in better preparation of medical students for learning in the clinical environment and support enhanced student experience and well-being.Method: An ethnographic approach included 120 hours of observations conducted in two wards hosting clinical placements for medical students, in a Scottish urban hospital. Additionally, individual interviews with 36 staff and students who populated these clinical sites, aimed to capture the lived experiences and perspectives around self-presentation, and impact of these performances in the workplace. Sensitising concepts from Goffman&amp;rsquo;s theory related to impression management served as priori guides in data analysis to identify prominent patterns.Results: We identified five key themes: students display a veneer of interest and engagement aligned to their understanding of the social norms in the CLE, creating a positive first impression on healthcare staff is a preventive practice adopted by students to avoid interprofessional conflicts, atypical personal front of overseas doctors and students impacts their social status, participatory learning with near peers involves less impression management burden and consequent stress, and understanding social rules of the CLE takes time and slows learning.Conclusion: The study findings reveal diverse ways in which medical students perform their role in the CLE through presentation of themselves and their activities to others. Engineering convincing and desired impressions is an affective and cognitive task for students, in their dual position of actor-performers and learners. Our findings indicate that certain personal fronts punctuate student learning, and we advocate for clinical workplaces to incorporate participatory learning opportunities, given their empowering benefits. Robust induction programmes and allowing learners to be authentically contributory in the CLE should ensure that diverse learners thrive in unfamiliar cultural spaces.


---
# Bridging the Gap with Convolutional networks: A Graph-based Vision Transformer with Sparsity for Training on Small Datasets from Scratch

## 用卷积网络弥合差距: 基于图形的视觉转换器，具有稀疏性，可从头开始训练小数据集

Link: https://www.researchsquare.com/article/rs-5364284/latest

Vision Transformers (ViTs) have achieved impressive results in large-scale image classification. However, when training from scratch on small datasets, there is still a significant performance gap between ViTs and Convolutional Neural Networks (CNNs), which is attributed to the lack of inductive bias. To address this issue, we propose a Graph-based Vision Transformer (GvT) that utilizes graph convolutional projection and graphpooling. In each block, queries and keys are calculated through graph convolutional projection based on the spatial adjacency matrix, while dot-product attention is used in another graph convolution to generate values. When using more attention heads, the queries and keys become lower-dimensional, making their dot product an uninformative matching function. To overcome this low-rank bottleneck in attention heads, we employ talkingheads technology based on bilinear pooled features and sparse selection of attention tensors. This allows interaction among filtered attention scores and enables each attention mechanism to depend on all queries and keys. Additionally, we apply graphpooling between two intermediate blocks to reduce the number of tokens and aggregate semantic information more effectively. Our experimental results show that GvT produces comparable or superior outcomes to deep convolutional networks and surpasses vision transformers without pre-training on large datasets. The code for our proposed model is publicly available on the website: GitHub/GvT.


---
# Optimizing Quality Tolerance Limits Monitoring in Clinical Trials Through Machine Learning Methods

## 通过机器学习方法优化临床试验中的质量容忍度监测

Link: https://www.researchsquare.com/article/rs-5374972/latest

The traditional clinical trial monitoring process, which relies heavily on site visits and manual review of accumulative patient data reported through Electronic Data Capture system, is time-consuming and resource-intensive. The recently emerged risk-based monitoring (RBM) and quality tolerance limit (QTL) framework offers a more efficient alternative solution to traditional SDV (source data verification) based quality assurance. These frameworks aim at proactively identifying systematic issues that impact patient safety and data integrity. In this paper, we proposed a machine learning enabled approach to facilitate real-time, automated monitoring of clinical trial QTL risk assessment. Unlike the traditional quality assurance process, where QTLs are evaluated based on single-source data and arbitrary defined fixed threshold, we utilize the QTL-ML framework to integrate information from multiple clinical domains to predict the clinical QTL of variety types at program, study, site and patient level. Moreover, our approach is assumption-free, relying not on historical expectations but on dynamically accumulating trial data to predict quality tolerance limit risks in an automated manner. Embedded within ICH-E6 recommended RBM principles, this innovative machine learning solution for QTL monitoring has the potential to transform sponsors&amp;rsquo; ability to protect patient safety, reduce trial duration, and lower trial costs.


---
# Predicting US Elections: A Machine Learning Approach

## 预测美国大选: 一种机器学习方法

Link: https://www.researchsquare.com/article/rs-5440358/latest

This study introduces a cutting-edge machine learning framework specif- ically developed to forecast the results of highly competitive political races, focusing on the 2018 Texas Senate election between Senator Ted Cruz and Beto O&amp;rsquo;Rourke. By utilizing a comprehensive dataset compris- ing historical election results from 2008 to 2016, granular polling data, and the relative importance of key campaign issues across counties, we employ advanced machine learning techniques, including Random Forest and Gradient Boosting, to construct a highly accurate predictive model. This hybrid approach effectively captures complex voting behavior pat- terns, enabling the model to predict Cruz&amp;rsquo;s statewide vote share with a minimal error margin of just 0.89%. The model&amp;rsquo;s precision not only under- scores the potential of machine learning to revolutionize election forecast- ing but also offers actionable insights that can significantly impact strate- gic decision-making in political campaigns. For Cruz&amp;rsquo;s team, the ability to predict electoral outcomes with such accuracy provides a competitive advantage in resource allocation, voter outreach, and issue prioritization. The model identifies critical factors&amp;mdash;such as the economy, immigration, and healthcare&amp;mdash;driving voter preferences, allowing the campaign to tailor its messaging and mobilization efforts more effectively. Furthermore, this framework can be adapted for future campaigns, providing a dynamic, data-driven tool that enhances Cruz&amp;rsquo;s ability to anticipate shifts in voter sentiment and strategically respond to emerging challenges.


---
# Literary Appreciation Through Online Learning Platforms

## 通过在线学习平台进行文学欣赏

Link: https://www.researchsquare.com/article/rs-5432325/latest

This study investigates how teachers integrate literature into online learning platforms during the pandemic. It examines which platforms are employed and the competence of teachers in administering literary appreciation. Conducted across three divisions in Surigao del Sur, the research used a mixed-method approach involving surveys, while quantitative data analysis included mean, percentage, and Pearson r correlation. Results revealed that common platforms, such as DepEd LMS, Google Classroom, and Quipperlink, are widely used, with most teachers demonstrating average digital competency. Despite the shift to online platforms, there was no significant relationship between teacher competency and student grades.


---
# The Impact of Artificial Intelligence-Powered Writing Assistance Systems on Metacognitive Writing Strategies

## 人工智能驱动的写作辅助系统对元认知写作策略的影响

Link: https://www.researchsquare.com/article/rs-5383311/latest

While writing plays an important role in the English as a foreign language (EFL) context as it is a tool to express ideas, apply what has been learned, stimulate higher-order skills, and be a measurement of writing proficiency, metacognitive writing strategies involve planning, monitoring, and evaluating stages, learners can improve writing performance and encourage self-regulation so that learners can control their learning process. As a facilitator for self-directed learning, AI-powered systems in writing may help learners manage their learning process and empower their metacognitive skills for writing. However, there has been a limited number of studies carried out on the impact of AI-powered writing assistance systems on metacognitive writing strategies in the EFL learning context. This study aims to examine the possible effects of integrating AI-powered writing tools on EFL learners&amp;rsquo; metacognitive writing strategies. It also compares AI-powered tools and conventional activities regarding the use of metacognitive strategies. In a quasi-experimental design, the study was conducted with 40 EFL learners. The study used a background questionnaire and the Metacognitive Writing Strategies Scale to collect data. The findings revealed that there was no significant difference between the use of traditional activities and AI-powered systems in terms of using metacognitive writing strategies. Therefore, it is recommended that the advantages and disadvantages of both means should be taken into consideration and that AI systems should be used as a supplementary tool to provide interaction, and immediate suggestions, and to address individual learning needs to enhance metacognitive strategy use.


---
# Federated Learning and Data Mining based Botnet Attack Detection Framework for Internet of Things

## 基于联邦学习和数据挖掘的物联网僵尸网络攻击检测框架

Link: https://www.researchsquare.com/article/rs-5365489/latest

Advancements in the Internet of Things (IoT) have resulted in numerous IoT devices, both standard and non-standard, being connected to the Internet. These devices have inherent vulnerabilities and are not patched or maintained well after deployment. This has created a wider threat surface due to the sheer number of devices present. Recently, botnet attacks exploited such weaknesses in IoT and carried out massive DDoS attacks on high profile targets. In this work, we propose a data mining and federated learning based solution to detect the individual attack stages in botnet attacks. The proposed solution extracts advanced features by mining anomalous patterns in network traffic using frequent itemset mining. Then, we use these features to train a distributed federated learning solution at gateways of networks while extracting the global distribution of attack patterns at a centralized security manager without violating user privacy. We carry out a set of experiments using OpenStack and a real IoT testbed to evaluate the performance of the proposed solution. The results demonstrate that the performance of the proposed privacy-preserving system is comparable to existing solutions that compromise user privacy.&nbsp;


---
# Investigating Parameter Initialization Techniques in Variational Quantum Circuits

## 变分量子电路中参数初始化技术的研究

Link: https://www.researchsquare.com/article/rs-5371051/latest

Variational Quantum Circuits (VQC)s have been widely explored to advance quantum circuits against classic models in various domains. Similar to classic machine-learning models, VQCs can be optimized through gradient-based approaches. However, the gradient of VQCs may dramatically vanish during the optimization process for various reasons. This issue, known as Barren Plateaus (BP)s, seriously hinders the advancement of quantum computing. To mitigate BPs, extensive efforts have been devoted to tackling this issue through different methods. In this work, we aim to investigate several circuit components through the lens of parameter initialization techniques. We study the performance and gradient variance of the different quantum circuit structures in combination with various initialization methods. This investigation will provide a better understanding of how the VQC structures behave under different initialization strategies for binary classification problems.


---
# EHHO-EL: Hybrid Method for Software defect Detection in Software Product Lines using Extended Harris Hawks Optimization and Ensemble Learning

## Ehho-el: 基于扩展Harris Hawks优化和集成学习的软件产品线中软件缺陷检测的混合方法

Link: https://www.researchsquare.com/article/rs-5379879/latest

Software Product Line (SPL) aims to reduce development costs and time while improving quality, but the complexity and involvement of multiple design teams often lead to defects and delays. Detecting and resolving defects in large-scale industrial SPLs remains a significant research area. This study proposes a hybrid approach that combines the Harris Hawks Optimization (HHO) algorithm with stacking-based ensemble learning for defect detection in SPLs. Enhanced by the Chaos Optimization Algorithm (COA) to avoid local optima and improve accuracy, the approach is evaluated on two datasets, LVAT and NASA, This study incorporates four datasets from each of these repositories. The experiment results show that the proposed method achieves detection accuracy rates of 92.7%, 91.1%, 96.3%, 98.4% for the LTS1, LTM2, LTL3, LTV4 and 97.91%, 99.01%, 94.21%, 90.93% for the CM1, JM1, KC1, PC1. Statistical tests confirm that this method offers superior accuracy and faster convergence compared to existing methods.


---
# Insulation optimization of aluminum profile for high-speed train based on RBF neural network model of sound insulation

## 基于RBF神经网络隔声模型的高速列车铝型材隔热优化

Link: https://www.researchsquare.com/article/rs-5352444/latest

Improving the sound insulation of the car body structure is an important technical means to reduce the noise inside the high-speed train. Without reducing the sound insulation of the structure, lowering the mass of the sound insulation structure is an important way to the lightweight of the high-speed train. The model based on radial basis function neural network (BRF) is established by taking the parameters such as the upper plate, the lower plate, the rib plate, the thickness of the aluminum profile and the angle of the rib plate as the design factors, and the weighted sound insulation Rw and the mass of the aluminum profile as the output response. The RBF model is used to optimize the sound insulation performance of the aluminum profile by using the hybrid algorithm of Multi-island Genetic Algorithm (MIGA) and nonlinear programming quadratic line (NLPQL), which greatly improves the optimization efficiency of the sound insulation performance of the aluminum profile. The sidewall aluminum profile of a high-speed train is optimized, and the Rw is increased by 3.6dB under the condition of constant mass. The mass is reduced by 18.5% when the Rw is constant.


---
# Deep Ransomware Detection through Dynamic Vulnerability Profiling for Real-Time Threat Identification

## 通过动态漏洞分析进行深度勒索软件检测，以进行实时威胁识别

Link: https://www.researchsquare.com/article/rs-5444729/latest

The escalating sophistication and frequency of ransomware attacks have demonstrated the necessity for innovative detection methodologies capable of adapting to rapidly evolving threats. Dynamic Vulnerability Profiling (DVP) emerges as a novel approach, dynamically modeling system vulnerabilities in response to the adaptive behaviors exhibited by contemporary ransomware. Through the integration of advanced machine learning algorithms and real-time data processing, DVP achieves high detection accuracy and resilience against sophisticated evasion techniques, addressing critical challenges inherent in current cybersecurity frameworks. Empirical evaluations demonstrate DVP's adaptability across various ransomware types and its efficient resource utilization, highlighting its potential for deployment in diverse operational environments. The insights derived from this study not only demonstrate the effectiveness of DVP but also contribute to the broader discourse on enhancing proactive defense mechanisms against ransomware threats, emphasizing the imperative of continuous innovation and adaptation in cybersecurity strategies.


---
# Predicting Surgical Complications Using Deep Learning Techniques

## 使用深度学习技术预测手术并发症

Link: https://www.researchsquare.com/article/rs-5426743/latest

ccurate prediction of surgical complications is crucial for optimizing treatment plans and improving patient outcomes. This paper investigates the application of deep learning models, particularly Convolutional Neural Networks (CNN) and Recurrent Neural Networks (RNN), for predicting the likelihood of complications based on patient and surgical data. Using a novel dataset with 1,500 instances and 20 features, we explore the performance of these models compared to traditional machine learning algorithms. Our results show that deep learning models provide superior accuracy, achieving 96


---
# Sparse Annotation is Sufficient for Bootstrapping Dense Neuron Segmentation

## 稀疏注释足以用于自举密集神经元分割

Link: https://www.researchsquare.com/article/rs-5339143/latest

Producing dense 3D reconstructions from biological imaging data is a challenging instance segmentation task that requires significant ground-truth training data for effective and accurate deep learning-based models. Generating training data requires intense human effort to annotate each instance of an object across serial section images. Our focus is on the especially complicated brain neuropil, comprising an extensive interdigitation of dendritic, axonal, and glial processes visualized through serial section electron microscopy. We developed a novel deep learning-based method to generate dense 3D segmentations rapidly from sparse 2D annotations of a few objects on single sections. Models trained on the rapidly generated segmentations achieved similar accuracy as those trained on expert dense ground-truth annotations. Human time to generate annotations was reduced by three orders of magnitude and could be produced by non-expert annotators. This capability will democratize generation of training data for large image volumes needed to achieve brain circuits and measures of circuit strengths.


---
# Predicting the Risk of Surgical Complications Using Machine Learning Models

## 使用机器学习模型预测手术并发症的风险

Link: https://www.researchsquare.com/article/rs-5426691/latest

Predicting the risk of surgical complications is essential to improve patient outcomes and optimize healthcare resources. In this paper, we propose the application of machine learning (ML) techniques to predict surgical risks based on pre-operative data. We used three supervised learning algorithms: Logistic Regression (LR), Random Forest (RF), and Support
Vector Machine (SVM). A stacked ensemble model combining these algorithms was also explored to enhance the prediction accuracy. The proposed ensemble model achieved a prediction accuracy of 94


---
# Predicting Patient Survival After Heart Failure Using Ensemble Learning Models

## 使用集成学习模型预测心力衰竭后的患者生存率

Link: https://www.researchsquare.com/article/rs-5426685/latest

Heart failure (HF) is the leading cause of global death from chronic diseases. Data mining using machine learning (ML) converts massive volumes of raw data created by healthcare institutions into meaningful information that can aid in making
predictions and crucial decisions. After an HF, collecting and analyzing follow-up data from patients is critical to monitor their health recovery. The aim of this study is to use ML and predict the survival possibility of patients after HF based on the follow-up data. Three supervised classifiers i.e., Random Forest (RF), XGBoost (XGB), and Decision Tree (DT) have been used in our study. Moreover, we proposed to design a supervised stacked ensemble learning model that can achieve a prediction accuracy, precision, recall, and F1 score of 99.98


---
# Cardiovascular Disease Prediction Using Machine Learning: An XGBoost Approach with
Hyperparameter Tuning

## 使用机器学习的心血管疾病预测: XGBoost方法
超参数调优

Link: https://www.researchsquare.com/article/rs-5426667/latest

Cardiovascular diseases (CVDs) are one of the leading causes of mortality worldwide. Early diagnosis and intervention are crucial to reducing the risk associated with these diseases. In this study, we propose a machine learning-based system for predicting cardiovascular disease using the Extreme Gradient Boosting (XGBoost) technique. We employ feature selection and hyperparameter optimization using random search to enhance the model&rsquo;s accuracy. The model&rsquo;s performance is evaluated through cross-validation and compared with other algorithms, including K-Nearest Neighbors (KNN), Na&uml;ıve Bayes, Support Vector Machine (SVM), and Random Forest. Experimental results show that our XGBoost-based model outperforms the other algorithms with an accuracy of 98% and an area under the ROC curve of 0.98.


---
# Performance of Generative AI Used by Chatgpt and Attention Mechanism for Remote Sensing Image Captioning (RSIC): a Comparative and Comprehensive Analysis

## Chatgpt和注意力机制用于遥感图像字幕 (RSIC) 的生成AI的性能: 比较和综合分析

Link: https://www.researchsquare.com/article/rs-5378004/latest

Remote Sensing Image Captioning (RSIC) is a challenging research area that generates descriptive captions of Remote Sensing Images (RSI). Recent advances in Deep Learning have contributed in RSIC and improved the capability to generate captions. In this paper, we present a comparative analysis of two prominent approaches: Channel Attention Mechanism and Generative AI. The channel attention mechanism emphasises on learning channel-wise with encoding-decoding, feature extraction and channel attention mechanism. On the other hand, Generative AI, particularly models like GPT, leverages large-scale pretraining to diverse datasets for generates coherent and contextually relevant captions. It extracts vision-based feature and contains auto-tokenizer within the Vi Encoder-Decoder. The objective of this study is to analytically understand the performance of Generative AI and other well-known schemes. The performance is evaluated in terms of caption quality, relevance, diversity, etc., and BLEU, METEOR, ROUGE and CIDEr are used as metrics in addition to Precision, Recall, F1-Score, F2-Score and Accuracy. We have used RSICD datasets and conducted experiments using Channel Attention Mechanism and Generative AI models. The experiment and analysis shed light on the advantages and disadvantages of each approach. It is observed that Generative AI performs well in terms of Precision, Recall, F1-Score, F2-Score and Accuracy. However, It has failed in describing the quantity of objects present in RSI and their Coordinates.

